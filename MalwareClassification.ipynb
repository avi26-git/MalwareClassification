{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this term project is to demonstrate your practical skills in implementing and deploying machine learning models for malware classification. The technical implementation of this project is comprised of three main tasks that need to be completed sequentially:\n",
    "\n",
    "Task 1 - Training: In this task, you will be creating and training a deep neural network based on the MalConv architecture to classify PE files as malware or benign. As for the dataset, you will be using the EMBER-2017 v2 ( https://github.com/endgameinc/ember ).\n",
    "\n",
    "If you explore the EMBER repository, you will find that it comes with a sample implementation of MalConv ( https://github.com/endgameinc/ember/tree/master/malconv ). This sample is a wonderful resource to base your implementation on. However, note that this code is 2 years (i.e., a lifetime in ML) old, and does not precisely conform to the requirements of this project.\n",
    "\n",
    "Implementation: The model must be implemented in Python 3.x using TensorFlow (1.x or 2.x) and Keras, and needs to be coded and documented in a Jupyter Notebook. Additionaly, add textual description blocks to the notebook to document and explain the different parts of your code.\n",
    "\n",
    "Training: This model may take a long time to train on your personal computers (from 7-8 hours to a couple of days, depending on the config), unless you already have a powerful NVIDIA GPU (1080 TI or better). Alternatively, you can use the cloud platforms to speed up the training: Google Colab or AWS Sagemaker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Model on EMBER Malware Dataset:\n",
    "The EMBER dataset is a collection of features from PE files that serve as a benchmark dataset for researchers.\n",
    "In this notebook, the EMBER-2017 v2 dataset is used which contains features from 1.1 million PE files scanned in or before 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Extraction:\n",
    "To use the dataset in this notebook, simple download and upload didn't work as the URL to download the dataset is detected as untrused by Google. So, downloading the EMBER 2017 v2 dataset to Colab notebook using wget command and double unzipping it to get the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-18 19:54:43--  https://pubdata.endgame.com/ember/ember_dataset_2017_2.tar.bz2\n",
      "Resolving pubdata.endgame.com (pubdata.endgame.com)... 64.250.189.21\n",
      "Connecting to pubdata.endgame.com (pubdata.endgame.com)|64.250.189.21|:443... connected.\n",
      "WARNING: cannot verify pubdata.endgame.com's certificate, issued by ‘CN=Go Daddy Secure Certificate Authority - G2,OU=http://certs.godaddy.com/repository/,O=GoDaddy.com\\\\, Inc.,L=Scottsdale,ST=Arizona,C=US’:\n",
      "  Issued certificate has expired.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1751237573 (1.6G) [application/octet-stream]\n",
      "Saving to: ‘ember_dataset_2017_2.tar.bz2’\n",
      "\n",
      "ember_dataset_2017_ 100%[===================>]   1.63G   111MB/s    in 15s     \n",
      "\n",
      "2021-12-18 19:54:58 (110 MB/s) - ‘ember_dataset_2017_2.tar.bz2’ saved [1751237573/1751237573]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://pubdata.endgame.com/ember/ember_dataset_2017_2.tar.bz2 --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompressing a .bz2 file\n",
    "!bzip2 -d ember_dataset_2017_2.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ember_2017_2/\n",
      "ember_2017_2/train_features_1.jsonl\n",
      "ember_2017_2/train_features_0.jsonl\n",
      "ember_2017_2/train_features_3.jsonl\n",
      "ember_2017_2/test_features.jsonl\n",
      "ember_2017_2/train_features_5.jsonl\n",
      "ember_2017_2/train_features_4.jsonl\n",
      "ember_2017_2/train_features_2.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Extracting from tar file\n",
    "!tar -xvf ember_dataset_2017_2.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the required dataset files are extracted.\n",
    "Now to work with the EMBER dataset, we need to clone its github repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/endgameinc/ember "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ember ember-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ember-master/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lief>=0.9.0\n",
      "  Downloading lief-0.11.5-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "     |████████████████████████████████| 3.9 MB 24.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.31.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.16.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.18.5)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (1.1.5)\n",
      "Collecting lightgbm>=2.2.3\n",
      "  Downloading lightgbm-3.3.1-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "     |████████████████████████████████| 2.0 MB 112.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (0.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pandas>=0.24.2->-r requirements.txt (line 4)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pandas>=0.24.2->-r requirements.txt (line 4)) (2021.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from lightgbm>=2.2.3->-r requirements.txt (line 5)) (1.5.3)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from lightgbm>=2.2.3->-r requirements.txt (line 5)) (0.36.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from scikit-learn>=0.20.3->-r requirements.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from scikit-learn>=0.20.3->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->-r requirements.txt (line 4)) (1.15.0)\n",
      "Installing collected packages: lightgbm, lief\n",
      "Successfully installed lief-0.11.5 lightgbm-3.3.1\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing ember.egg-info/PKG-INFO\n",
      "writing dependency_links to ember.egg-info/dependency_links.txt\n",
      "writing requirements to ember.egg-info/requires.txt\n",
      "writing top-level names to ember.egg-info/top_level.txt\n",
      "reading manifest file 'ember.egg-info/SOURCES.txt'\n",
      "writing manifest file 'ember.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/ember\n",
      "copying build/lib/ember/__init__.py -> build/bdist.linux-x86_64/egg/ember\n",
      "copying build/lib/ember/features.py -> build/bdist.linux-x86_64/egg/ember\n",
      "byte-compiling build/bdist.linux-x86_64/egg/ember/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/ember/features.py to features.cpython-36.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying ember.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying ember.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying ember.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying ember.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying ember.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/ember-0.1.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing ember-0.1.0-py3.6.egg\n",
      "Copying ember-0.1.0-py3.6.egg to /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Adding ember 0.1.0 to easy-install.pth file\n",
      "\n",
      "Installed /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ember-0.1.0-py3.6.egg\n",
      "Processing dependencies for ember==0.1.0\n",
      "Searching for scikit-learn==0.24.1\n",
      "Best match: scikit-learn 0.24.1\n",
      "Adding scikit-learn 0.24.1 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Searching for lightgbm==3.3.1\n",
      "Best match: lightgbm 3.3.1\n",
      "Adding lightgbm 3.3.1 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Searching for pandas==1.1.5\n",
      "Best match: pandas 1.1.5\n",
      "Adding pandas 1.1.5 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Searching for numpy==1.18.5\n",
      "Best match: numpy 1.18.5\n",
      "Adding numpy 1.18.5 to easy-install.pth file\n",
      "Installing f2py script to /home/ec2-user/anaconda3/envs/tensorflow_p36/bin\n",
      "Installing f2py3 script to /home/ec2-user/anaconda3/envs/tensorflow_p36/bin\n",
      "Installing f2py3.6 script to /home/ec2-user/anaconda3/envs/tensorflow_p36/bin\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Searching for tqdm==4.62.3\n",
      "Best match: tqdm 4.62.3\n",
      "Adding tqdm 4.62.3 to easy-install.pth file\n",
      "Installing tqdm script to /home/ec2-user/anaconda3/envs/tensorflow_p36/bin\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Searching for lief==0.11.5\n",
      "Best match: lief 0.11.5\n",
      "Adding lief 0.11.5 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Searching for threadpoolctl==2.1.0\n",
      "Best match: threadpoolctl 2.1.0\n",
      "Adding threadpoolctl 2.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Searching for scipy==1.5.3\n",
      "Best match: scipy 1.5.3\n",
      "Adding scipy 1.5.3 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Searching for joblib==1.0.1\n",
      "Best match: joblib 1.0.1\n",
      "Adding joblib 1.0.1 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Searching for wheel==0.36.2\n",
      "Best match: wheel 0.36.2\n",
      "Adding wheel 0.36.2 to easy-install.pth file\n",
      "Installing wheel script to /home/ec2-user/anaconda3/envs/tensorflow_p36/bin\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Searching for pytz==2021.1\n",
      "Best match: pytz 2021.1\n",
      "Adding pytz 2021.1 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Searching for python-dateutil==2.8.1\n",
      "Best match: python-dateutil 2.8.1\n",
      "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Searching for six==1.15.0\n",
      "Best match: six 1.15.0\n",
      "Adding six 1.15.0 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages\n",
      "Finished processing dependencies for ember==0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.11.5-37bc2c9 found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n",
      "Vectorizing training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900000/900000 [03:18<00:00, 4528.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:45<00:00, 4414.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha256</th>\n",
       "      <th>appeared</th>\n",
       "      <th>label</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...</td>\n",
       "      <td>2006-12</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d4206650743b3d519106dea10a38a55c30467c3d9f7875...</td>\n",
       "      <td>2006-12</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...</td>\n",
       "      <td>2007-01</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7f513818bcc276c531af2e641c597744da807e21cc1160...</td>\n",
       "      <td>2007-02</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...</td>\n",
       "      <td>2007-02</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099995</th>\n",
       "      <td>fffe314f23cee3a68ccab272934877d3bc18ec3bd905df...</td>\n",
       "      <td>2017-12</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099996</th>\n",
       "      <td>fffe7a1b23e04facc9ca91a93ac4a34e8b3040e023dbde...</td>\n",
       "      <td>2017-12</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099997</th>\n",
       "      <td>fffe801f51e7ec931515aa49a3d157a9c0fbcdca8c9d80...</td>\n",
       "      <td>2017-12</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099998</th>\n",
       "      <td>fffe92f9593649c4a7050302368189de45e2c1c06b04ea...</td>\n",
       "      <td>2017-12</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099999</th>\n",
       "      <td>ffffb259a4c5e25ae1437af59caafb718cf8879187cc8c...</td>\n",
       "      <td>2017-12</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    sha256 appeared  label  \\\n",
       "0        0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...  2006-12      0   \n",
       "1        d4206650743b3d519106dea10a38a55c30467c3d9f7875...  2006-12      0   \n",
       "2        c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...  2007-01      0   \n",
       "3        7f513818bcc276c531af2e641c597744da807e21cc1160...  2007-02      0   \n",
       "4        ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...  2007-02      0   \n",
       "...                                                    ...      ...    ...   \n",
       "1099995  fffe314f23cee3a68ccab272934877d3bc18ec3bd905df...  2017-12      0   \n",
       "1099996  fffe7a1b23e04facc9ca91a93ac4a34e8b3040e023dbde...  2017-12      1   \n",
       "1099997  fffe801f51e7ec931515aa49a3d157a9c0fbcdca8c9d80...  2017-12      0   \n",
       "1099998  fffe92f9593649c4a7050302368189de45e2c1c06b04ea...  2017-12      1   \n",
       "1099999  ffffb259a4c5e25ae1437af59caafb718cf8879187cc8c...  2017-12      1   \n",
       "\n",
       "        subset  \n",
       "0        train  \n",
       "1        train  \n",
       "2        train  \n",
       "3        train  \n",
       "4        train  \n",
       "...        ...  \n",
       "1099995   test  \n",
       "1099996   test  \n",
       "1099997   test  \n",
       "1099998   test  \n",
       "1099999   test  \n",
       "\n",
       "[1100000 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ember\n",
    "ember.create_vectorized_features(\"/home/ec2-user/SageMaker/MalwareClassification/ember_2017_2/\")\n",
    "ember.create_metadata(\"/home/ec2-user/SageMaker/MalwareClassification/ember_2017_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha256</th>\n",
       "      <th>appeared</th>\n",
       "      <th>label</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...</td>\n",
       "      <td>2006-12</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d4206650743b3d519106dea10a38a55c30467c3d9f7875...</td>\n",
       "      <td>2006-12</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...</td>\n",
       "      <td>2007-01</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7f513818bcc276c531af2e641c597744da807e21cc1160...</td>\n",
       "      <td>2007-02</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...</td>\n",
       "      <td>2007-02</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sha256 appeared  label subset\n",
       "0  0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...  2006-12      0  train\n",
       "1  d4206650743b3d519106dea10a38a55c30467c3d9f7875...  2006-12      0  train\n",
       "2  c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...  2007-01      0  train\n",
       "3  7f513818bcc276c531af2e641c597744da807e21cc1160...  2007-02      0  train\n",
       "4  ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...  2007-02      0  train"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ember\n",
    "data_path = \"/home/ec2-user/SageMaker/MalwareClassification/ember_2017_2/\"\n",
    "emberdf = ember.read_metadata(data_path)\n",
    "emberdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.11.5-37bc2c9 found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n"
     ]
    }
   ],
   "source": [
    "X_train0, y_train0, X_test0, y_test0 = ember.read_vectorized_features(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[1.4676122e-02, 4.2218715e-03, 3.9226813e-03, ..., 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        [7.2290748e-02, 1.4057922e-02, 1.1037279e-02, ..., 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        [1.8452372e-01, 3.1307504e-02, 5.6928140e-03, ..., 4.4229600e+05,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        ...,\n",
       "        [3.2558188e-01, 5.2042645e-03, 4.0974934e-03, ..., 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        [5.0731432e-01, 9.9041909e-03, 5.1698429e-03, ..., 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00],\n",
       "        [6.8576080e-01, 4.2310823e-03, 4.0683481e-03, ..., 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((900000, 2381), (900000,), (200000, 2381), (200000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of the dataset\n",
    "X_train0.shape, y_train0.shape, X_test0.shape, y_test0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing:\n",
    "It is known that the EMBER train dataset has three sample categories, namels unlabled, benign and malicious.\n",
    "They are represented as -1, 0 and 1 respectively.\n",
    "\n",
    "But it can be seen that the test dataset has only benign and malicious samples.\n",
    "Ignoring the unlabled samples from the train dataset for the better performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((900000, 2381), (900000, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Creating dataframes of X_train & y_train\n",
    "X_train0 = pd.DataFrame(X_train0)\n",
    "y_train0 = pd.DataFrame(y_train0)\n",
    "X_train0.shape, y_train0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -1.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unique labels in the train dataset \n",
    "y_train0[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((900000, 2382), (900000, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining features and lables of train dataset\n",
    "X_train0[2381] = y_train0[0]\n",
    "X_train0.shape, y_train0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -1.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the presence of unique lables in the combined dataframe\n",
    "X_train0[2381].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the unlabeled rows from the dataframe\n",
    "\n",
    "X_train0.drop(X_train0[(X_train0[2381] == -1)].index, inplace = True)\n",
    "y_train0.drop(y_train0[(y_train0[0] == -1)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600000, 2382), (600000, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train0.shape, y_train0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600000, 2381), (600000, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reconstructing the X_train dataframe\n",
    "X_train0.drop([2381], axis =1, inplace=True)\n",
    "X_train0.shape, y_train0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is huge and takes lot to time for vectorizing and creating metadata for every runtime execution.\n",
    "So, create pickle files for the training and testing samples to store them in the system.\n",
    "By downloading and storing these pickle files, we avoid the execution of the former lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling the datasets\n",
    "pd.DataFrame(X_train0).to_pickle(\"./X_train.pkl\")\n",
    "pd.DataFrame(y_train0).to_pickle(\"./y_train.pkl\")\n",
    "pd.DataFrame(X_test0).to_pickle(\"./X_test.pkl\")\n",
    "pd.DataFrame(y_test0).to_pickle(\"./y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting training data from pickle files\n",
    "X_trainp = pd.read_pickle(\"./X_train.pkl\")\n",
    "y_trainp = pd.read_pickle(\"./y_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting testing data from pickle files\n",
    "X_testp =pd.read_pickle(\"./X_test.pkl\")\n",
    "y_testp = pd.read_pickle(\"./y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600000, 2381), (600000, 1), (200000, 2381), (200000, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of the dataset\n",
    "X_trainp.shape, y_trainp.shape, X_testp.shape, y_testp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we noticed that the above lines of code used around 25GB RAM.\n",
    "So, even though the datasets are pickled, the RAM crashes.\n",
    "Hence, the alternative for this is to create HDF5 files.\n",
    "The h5py package is a Pythonic interface to the HDF5 binary data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Loading X_train data to HDF5 file\n",
    "h50 = h5py.File('X_train0.h5', 'w')\n",
    "h50.create_dataset('X_train0', data=X_train0)\n",
    "h50.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading y_train data to HDF5 file\n",
    "h51 = h5py.File('y_train0.h5', 'w')\n",
    "h51.create_dataset('y_train0', data=y_train0)\n",
    "h51.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading X_test data to HDF5 file\n",
    "h52 = h5py.File('X_test0.h5', 'w')\n",
    "h52.create_dataset('X_test0', data=X_test0)\n",
    "h52.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading y_test data to HDF5 file\n",
    "h53 = h5py.File('y_test0.h5', 'w')\n",
    "h53.create_dataset('y_test0', data=y_test0)\n",
    "h53.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 2381)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the X_train data from h5 files \n",
    "import h5py\n",
    "Xh5 = h5py.File('./X_train0.h5','r')\n",
    "X_train = Xh5['X_train0']\n",
    "X_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading y_train data from h5 files\n",
    "import h5py\n",
    "yh5 = h5py.File('./y_train0.h5','r')\n",
    "y_train = yh5['y_train0']\n",
    "y_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 2381)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading X_test data from h5 files\n",
    "import h5py\n",
    "Xth5 = h5py.File('./X_test0.h5','r')\n",
    "X_test = Xth5['X_test0']\n",
    "X_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading y_test data from h5 files\n",
    "import h5py\n",
    "yth5 = h5py.File('./y_test0.h5','r')\n",
    "y_test = yth5['y_test0']\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The features of this dataset are scaled on different scalars.\n",
    "We picked RobustScalar to do the feature scaling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features inorder to improve the performance of the model\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rs = RobustScaler()\n",
    "Xtrain_rs = rs.fit_transform(X_train)\n",
    "Xtest_rs = rs.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading scaled X_train data to HDF5 file\n",
    "h54 = h5py.File('Xtrain_rs.h5', 'w')\n",
    "h54.create_dataset('Xtrain_rs', data=Xtrain_rs)\n",
    "h54.close()\n",
    "\n",
    "#Storing the h5 files to GDrive\n",
    "# !cp ./Xtrain_rs.h5 ./gdrive/My\\ Drive/Pickle_Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading scaled X_test data to HDF5 file\n",
    "h55 = h5py.File('Xtest_rs.h5', 'w')\n",
    "h55.create_dataset('Xtest_rs', data=Xtest_rs)\n",
    "h55.close()\n",
    "\n",
    "#Storing the h5 files to GDrive\n",
    "# !cp ./Xtest_rs.h5 ./gdrive/My\\ Drive/Pickle_Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 2381)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Xtrain_rs data from h5 files\n",
    "import h5py\n",
    "Xrsh5 = h5py.File('./Xtrain_rs.h5','r')\n",
    "Xtrain_rs = Xrsh5['Xtrain_rs']\n",
    "Xtrain_rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 2381)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Xtest_rs data from h5 files\n",
    "import h5py\n",
    "Xtrsh5 = h5py.File('./Xtest_rs.h5','r')\n",
    "Xtest_rs = Xtrsh5['Xtest_rs']\n",
    "Xtest_rs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Arcitecture & Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
